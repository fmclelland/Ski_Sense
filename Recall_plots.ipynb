{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "- Import the data\n",
    "- Segment data into 2 turn segments\n",
    "- Calculate the FFT\n",
    "- Solve for the DFT ratio\n",
    "- Calculate the SVD of the dft ratio, dft(strain), dft(angular velocity).\n",
    "- Genrate scatter plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating derivatives and integrals\n",
    "import math\n",
    "import pynumdiff\n",
    "import scipy\n",
    "import cvxpy as cp\n",
    "# import umap.umap_ as umap\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# For processing data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For calculating frequency content\n",
    "from scipy import fft\n",
    "from scipy import signal\n",
    "\n",
    "# For plotting signals\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a mean subtraction to ensure data is centered around the zero.\n",
    "def sig_mean_df(df, columns,time):\n",
    "    df_mean = pd.DataFrame(columns=columns)\n",
    "    for i in range(len(columns)):\n",
    "        signal = df[columns[i]]\n",
    "        signal_mean = np.mean(signal)\n",
    "        signal_use = (signal-signal_mean)\n",
    "        df_mean[columns[i]] = signal_use\n",
    "    df_mean[time] = df[time]\n",
    "    \n",
    "    return(df_mean)\n",
    "\n",
    "# Smooth your data like butter. Return original dataframe with smoothed data appended.\n",
    "def smooth_butter(df, params, dt, col_names):\n",
    "    for i in range(np.size(col_names)):\n",
    "        df[col_names[i] + '_hat'], df[col_names[i] + '_dt'] = pynumdiff.smooth_finite_difference.butterdiff(df[col_names[i]], dt, params, options={'iterate': False})\n",
    "    return(df)\n",
    "\n",
    "# Smooth your data and return only smooth signals.\n",
    "def smooth_only(df, params, dt, col_names):\n",
    "    df_new = pd.DataFrame()\n",
    "    for i in range(len(col_names)):\n",
    "        smooth_data, smooth_derivative = pynumdiff.smooth_finite_difference.butterdiff(df[col_names[i]], dt, params, options={'iterate': False})\n",
    "        df_new[col_names[i]] = smooth_data \n",
    "    return(df_new)\n",
    "\n",
    "# Calculate the DFT of a data set and return a data frame including the DFT data\n",
    "def simple_fft_array(array,dt):\n",
    "    fft_df = pd.DataFrame()\n",
    "    N = np.shape(array)[0]\n",
    "    yf = 2.0/N * np.abs(fft.fft(array)[0:N//2])\n",
    "    xf = fft.fftfreq(N,dt)[:N//2]\n",
    "    fft_df['array_fft'] = yf\n",
    "    fft_df['frequency'] = xf\n",
    "    return(fft_df)\n",
    "\n",
    "# Take the FFT of a dataset and return a new data frame containing the DFT data.\n",
    "def fft_of_data_df(df,dt,columns):\n",
    "    fft_df = pd.DataFrame()\n",
    "    for i in range(len(columns)):\n",
    "        data = df[columns[i]]\n",
    "        data = np.array(data)\n",
    "        N = data.shape[0] # Length of data.\n",
    "        yf = 2.0/N * np.abs(fft.fft(data)[0:N//2])# Divide to bring the manitude to the magnitude of the orginal sinewave.\n",
    "        # Remember the the y-data will be complex so you have to take the absolute value.\n",
    "        xf = fft.fftfreq(N,dt)[:N//2]# Only take the first half of the data.\n",
    "        fft_df[columns[i]+'_fft'] = yf\n",
    "        if i == len(columns)-1:\n",
    "            fft_df['Frequency'] = xf\n",
    "    return(fft_df)\n",
    "\n",
    "# Use query to extract segments of data.\n",
    "def segment_data(df, column_title, lower_lim, upper_lim):\n",
    "    q = column_title + ' < ' + str(upper_lim) + ' and ' + column_title + ' > ' + str(lower_lim)\n",
    "    new_df = df.query(q)\n",
    "    return(new_df)\n",
    "\n",
    "# Segment the data into desired segment size and take the FFT of the signals\n",
    "# Return a new data frame.\n",
    "def segment_create_new_df_for_each_sg(list_of_dfs,strain_gauge,chunck_size,snow_types,dt):\n",
    "    lower_freq = 1/60\n",
    "    high_freq = 150\n",
    "    df_new = pd.DataFrame()\n",
    "#     dataframes = np.shape(list_of_dfs[0])[0]\n",
    "    data_frames = len(snow_types)-1\n",
    "    for j in range(data_frames):\n",
    "            df_column = list_of_dfs[j][strain_gauge]\n",
    "            n = len(df_column)//chunk_size\n",
    "            for i in range(n):\n",
    "                data = df_column[i*chunk_size:(i+1)*chunk_size] # \"data\" is the segmented timeseries. \n",
    "                df_new[df_column.name+'_'+str(i+1)+'_'+snow_types[j]] = data.reset_index(drop=True)\n",
    "    df_new = fft_of_data_df(df_new,dt,df_new.columns.tolist())\n",
    "    # Take every 5th entry.\n",
    "    df_new = segment_data(df_new,'Frequency',lower_freq,high_freq) #.iloc[0::2,:].reset_index(drop=True)\n",
    "    return(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Load data and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 08:38:06,769 [INFO] NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "# Import ski data\n",
    "df_grmr = pd.read_hdf('DATA/df_grmr.hdf')\n",
    "df_grmr1 = pd.read_hdf('DATA/df_grmr1.hdf')\n",
    "df_grmr2 = pd.read_hdf('DATA/df_grmr2.hdf')\n",
    "df_grmr3 = pd.read_hdf('DATA/df_grmr3.hdf')\n",
    "df_grmr4 = pd.read_hdf('DATA/df_grmr4.hdf')\n",
    "df_grmr5 = pd.read_hdf('DATA/df_grmr5.hdf')\n",
    "\n",
    "df_pwdr = pd.read_hdf('DATA/df_pwdr.hdf')\n",
    "df_pwdr1 = pd.read_hdf('DATA/df_pwdr1.hdf')\n",
    "\n",
    "df_icy = pd.read_hdf('DATA/df_icy.hdf')\n",
    "\n",
    "df_s = pd.read_hdf('DATA/df_s.hdf')\n",
    "df_s1 = pd.read_hdf('DATA/df_s1.hdf')\n",
    "df_s2 = pd.read_hdf('DATA/df_s2.hdf')\n",
    "df_s3 = pd.read_hdf('DATA/df_s3.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frames that are mean subtracted.\n",
    "columns = ['sg3','sg4','sg6','sgc','sg7','sg12','sg15','imu_ax','imu_ay','imu_az','imu_wx','imu_wy','imu_wz'] # Columns to use.\n",
    "df_grmr_mean  = sig_mean_df(df_grmr,columns,'time_imu')\n",
    "df_grmr1_mean = sig_mean_df(df_grmr1,columns,'time_imu')\n",
    "df_grmr2_mean = sig_mean_df(df_grmr2,columns,'time_imu')\n",
    "df_grmr3_mean = sig_mean_df(df_grmr3,columns,'time_imu')\n",
    "df_grmr4_mean = sig_mean_df(df_grmr4,columns,'time_imu')\n",
    "df_grmr5_mean = sig_mean_df(df_grmr5,columns,'time_imu')\n",
    "df_pwdr_mean  = sig_mean_df(df_pwdr,columns,'time_imu')\n",
    "df_pwdr1_mean = sig_mean_df(df_pwdr1,columns,'time_imu')\n",
    "df_icy_mean   = sig_mean_df(df_icy,columns,'time_imu')\n",
    "df_s_mean     = sig_mean_df(df_s,columns,'time_imu')\n",
    "df_s1_mean    = sig_mean_df(df_s1,columns,'time_imu')\n",
    "df_s2_mean    = sig_mean_df(df_s2,columns,'time_imu')\n",
    "df_s3_mean    = sig_mean_df(df_s3,columns,'time_imu')\n",
    "all_mean_time = [df_grmr_mean,df_grmr1_mean,df_grmr2_mean,df_grmr3_mean,df_grmr4_mean,df_grmr5_mean,\n",
    "                df_pwdr_mean,df_pwdr1_mean,\n",
    "                df_icy_mean,\n",
    "                df_s_mean,df_s1_mean,df_s2_mean,df_s3_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Plot strain and smoothed angular velocity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Segment the data\n",
    "- Segment into 2 turn segments\n",
    "- Take the FFT of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the original non normalized strain data.\n",
    "# And take the FFT of the signals.\n",
    "dt = 0.001 # Sample period\n",
    "snow_types = ['groomer_slow','groomer_slow1','groomer_fast','groomer_fast1','groomer_slow2','groomer_fast2','powder_1',\n",
    "              'powder_5','icy_fast','icy1_fast','slushy_fast','slushy_slow','slushy_fast1','slushy_slow1']\n",
    "strain_gauge = ['sg3','sg4','sg12','sg6','sgc','sg7','sg15']\n",
    "chunk_size = 10000\n",
    "sg3_segmented_all_snow = segment_create_new_df_for_each_sg(all_mean_time,strain_gauge[0],chunk_size,snow_types,dt)\n",
    "sg4_segmented_all_snow = segment_create_new_df_for_each_sg(all_mean_time,strain_gauge[1],chunk_size,snow_types,dt)\n",
    "sg12_segmented_all_snow = segment_create_new_df_for_each_sg(all_mean_time,strain_gauge[2],chunk_size,snow_types,dt)\n",
    "sg6_segmented_all_snow = segment_create_new_df_for_each_sg(all_mean_time,strain_gauge[3],chunk_size,snow_types,dt)\n",
    "sgc_segmented_all_snow = segment_create_new_df_for_each_sg(all_mean_time,strain_gauge[4],chunk_size,snow_types,dt)\n",
    "sg7_segmented_all_snow = segment_create_new_df_for_each_sg(all_mean_time,strain_gauge[5],chunk_size,snow_types,dt)\n",
    "sg15_segmented_all_snow = segment_create_new_df_for_each_sg(all_mean_time,strain_gauge[6],chunk_size,snow_types,dt)\n",
    "\n",
    "strain_segmented_all_fft = [sg3_segmented_all_snow.iloc[:,0:65],sg12_segmented_all_snow.iloc[:,0:65],sg6_segmented_all_snow.iloc[:,0:65],\n",
    "                           sgc_segmented_all_snow.iloc[:,0:65],sg7_segmented_all_snow.iloc[:,0:65],sg15_segmented_all_snow.iloc[:,0:65]]\n",
    "\n",
    "ang_velx_all_snow = segment_create_new_df_for_each_sg(all_mean_time,'imu_wx',chunk_size,snow_types,dt)\n",
    "ang_vely_all_snow = segment_create_new_df_for_each_sg(all_mean_time,'imu_wy',chunk_size,snow_types,dt)\n",
    "ang_velz_all_snow = segment_create_new_df_for_each_sg(all_mean_time,'imu_wz',chunk_size,snow_types,dt)\n",
    "\n",
    "angular_vel_all = [ang_velx_all_snow,ang_vely_all_snow,ang_velz_all_snow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Solve for the DFT ratio \n",
    "- Do this using various \"inputs\"\n",
    "- Inputs are the 3 axes of angular velocity measured from the IMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the DFT ratio for x axis\n",
    "gamma = 0.8 # Weighting parameter for total variance constraint.\n",
    "all_b3 = []\n",
    "all_b12 = []\n",
    "all_b6 = []\n",
    "all_bc = []\n",
    "all_b7 = []\n",
    "all_b15 = []\n",
    "\n",
    "for i in range(np.shape(ang_velx_all_snow)[1]-1):\n",
    "    in_data = np.array(ang_velx_all_snow.iloc[:,i]).reshape((np.shape(ang_velx_all_snow.iloc[:,i])[0],1)) # The input will be the same across all strain gauges.\n",
    "    out_data3 = np.array(sg3_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg3_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data12 = np.array(sg12_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg12_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data6 = np.array(sg6_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg6_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_datac = np.array(sgc_segmented_all_snow.iloc[:,i]).reshape((np.shape(sgc_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data7 = np.array(sg7_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg7_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data15 = np.array(sg15_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg15_segmented_all_snow.iloc[:,i])[0],1))\n",
    "\n",
    "    bode3 = cp.Variable(np.shape(in_data)[0]) # Numerical bode plot that resets every time, but the values are appended. \n",
    "    bode12 = cp.Variable(np.shape(in_data)[0])\n",
    "    bode6 = cp.Variable(np.shape(in_data)[0])\n",
    "    bodec = cp.Variable(np.shape(in_data)[0])\n",
    "    bode7 = cp.Variable(np.shape(in_data)[0])\n",
    "    bode15 = cp.Variable(np.shape(in_data)[0])\n",
    "\n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode3) - out_data3[:,0])) + gamma*cp.tv(bode3)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b3.append(bode3.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode12) - out_data12[:,0])) + gamma*cp.tv(bode12)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b12.append(bode12.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode6) - out_data6[:,0])) + gamma*cp.tv(bode6)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b6.append(bode6.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bodec) - out_datac[:,0])) + gamma*cp.tv(bodec)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_bc.append(bodec.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode7) - out_data7[:,0])) + gamma*cp.tv(bode7)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b7.append(bode7.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode15) - out_data15[:,0])) + gamma*cp.tv(bode15)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b15.append(bode15.value)\n",
    "    \n",
    "all_bode_datax = [np.transpose(all_b3),np.transpose(all_b12),np.transpose(all_b6),np.transpose(all_bc),np.transpose(all_b7),np.transpose(all_b15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y axis\n",
    "all_b3 = []\n",
    "all_b12 = []\n",
    "all_b6 = []\n",
    "all_bc = []\n",
    "all_b7 = []\n",
    "all_b15 = []\n",
    "\n",
    "for i in range(np.shape(ang_vely_all_snow)[1]-1):\n",
    "    in_data = np.array(ang_vely_all_snow.iloc[:,i]).reshape((np.shape(ang_vely_all_snow.iloc[:,i])[0],1)) # The input will be the same across all strain gauges.\n",
    "    out_data3 = np.array(sg3_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg3_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data12 = np.array(sg12_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg12_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data6 = np.array(sg6_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg6_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_datac = np.array(sgc_segmented_all_snow.iloc[:,i]).reshape((np.shape(sgc_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data7 = np.array(sg7_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg7_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data15 = np.array(sg15_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg15_segmented_all_snow.iloc[:,i])[0],1))\n",
    "\n",
    "    bode3 = cp.Variable(np.shape(in_data)[0]) # Numerical bode plot that resets every time, but the values are appended. \n",
    "    bode12 = cp.Variable(np.shape(in_data)[0])\n",
    "    bode6 = cp.Variable(np.shape(in_data)[0])\n",
    "    bodec = cp.Variable(np.shape(in_data)[0])\n",
    "    bode7 = cp.Variable(np.shape(in_data)[0])\n",
    "    bode15 = cp.Variable(np.shape(in_data)[0])\n",
    "\n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode3) - out_data3[:,0])) + gamma*cp.tv(bode3)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b3.append(bode3.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode12) - out_data12[:,0])) + gamma*cp.tv(bode12)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b12.append(bode12.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode6) - out_data6[:,0])) + gamma*cp.tv(bode6)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b6.append(bode6.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bodec) - out_datac[:,0])) + gamma*cp.tv(bodec)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_bc.append(bodec.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode7) - out_data7[:,0])) + gamma*cp.tv(bode7)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b7.append(bode7.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode15) - out_data15[:,0])) + gamma*cp.tv(bode15)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b15.append(bode15.value)\n",
    "\n",
    "all_bode_datay = [np.transpose(all_b3),np.transpose(all_b12),np.transpose(all_b6),np.transpose(all_bc),np.transpose(all_b7),np.transpose(all_b15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z axis\n",
    "all_b3 = []\n",
    "all_b12 = []\n",
    "all_b6 = []\n",
    "all_bc = []\n",
    "all_b7 = []\n",
    "all_b15 = []\n",
    "\n",
    "for i in range(np.shape(ang_velz_all_snow)[1]-1):\n",
    "    in_data = np.array(ang_velz_all_snow.iloc[:,i]).reshape((np.shape(ang_velz_all_snow.iloc[:,i])[0],1)) # The input will be the same across all strain gauges.\n",
    "    out_data3 = np.array(sg3_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg3_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data12 = np.array(sg12_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg12_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data6 = np.array(sg6_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg6_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_datac = np.array(sgc_segmented_all_snow.iloc[:,i]).reshape((np.shape(sgc_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data7 = np.array(sg7_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg7_segmented_all_snow.iloc[:,i])[0],1))\n",
    "    out_data15 = np.array(sg15_segmented_all_snow.iloc[:,i]).reshape((np.shape(sg15_segmented_all_snow.iloc[:,i])[0],1))\n",
    "\n",
    "    bode3 = cp.Variable(np.shape(in_data)[0]) # Numerical bode plot that resets every time, but the values are appended. \n",
    "    bode12 = cp.Variable(np.shape(in_data)[0])\n",
    "    bode6 = cp.Variable(np.shape(in_data)[0])\n",
    "    bodec = cp.Variable(np.shape(in_data)[0])\n",
    "    bode7 = cp.Variable(np.shape(in_data)[0])\n",
    "    bode15 = cp.Variable(np.shape(in_data)[0])\n",
    "\n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode3) - out_data3[:,0])) + gamma*cp.tv(bode3)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b3.append(bode3.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode12) - out_data12[:,0])) + gamma*cp.tv(bode12)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b12.append(bode12.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode6) - out_data6[:,0])) + gamma*cp.tv(bode6)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b6.append(bode6.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bodec) - out_datac[:,0])) + gamma*cp.tv(bodec)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_bc.append(bodec.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode7) - out_data7[:,0])) + gamma*cp.tv(bode7)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b7.append(bode7.value)\n",
    "    \n",
    "    L = cp.sum(cp.square(cp.multiply(in_data[:,0],bode15) - out_data15[:,0])) + gamma*cp.tv(bode15)\n",
    "    objective = cp.Minimize(L)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve()\n",
    "    all_b15.append(bode15.value)\n",
    "    \n",
    "all_bode_dataz = [np.transpose(all_b3),np.transpose(all_b12),np.transpose(all_b6),np.transpose(all_bc),np.transpose(all_b7),np.transpose(all_b15)]\n",
    "\n",
    "# Stack all the ratios together to take advantage of all three axes. \n",
    "all_bode_data = np.hstack((all_bode_datax,all_bode_datay,all_bode_dataz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create different combinations of strain gagues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = all_bode_data[0]\n",
    "c12 = all_bode_data[1]\n",
    "c6 = all_bode_data[2]\n",
    "cc = all_bode_data[3]\n",
    "c7 = all_bode_data[4]\n",
    "c15 = all_bode_data[5]\n",
    "bode_stacked_all = np.hstack((c3,c12,c6,cc,c7,c15))\n",
    "bode_stacked_12_7 = np.hstack((c12,c7))\n",
    "bode_stacked_3_15 = np.hstack((c3,c15))\n",
    "bode_stacked_3_15_7 = np.hstack((c3,c15,c7))\n",
    "bode_stacked_3_7 = np.hstack((c3,c7))\n",
    "bode_stacked_3_6 = np.hstack((c3,c6))\n",
    "bode_stacked_6_7 = np.hstack((c6,c7))\n",
    "bode_stacked_3_12 = np.hstack((c3,c12))\n",
    "bode_stacked_7_15 = np.hstack((c7,c15))\n",
    "bode_stacked_12_15 = np.hstack((c12,c15))\n",
    "bode_stacked_c = cc\n",
    "\n",
    "# bode_stacked_log = np.log10(bode_stacked)\n",
    "U_stack,S_stack,V_stack = np.linalg.svd(bode_stacked_c,full_matrices=False)\n",
    "\n",
    "U_stack_l_all,S_stack_l_all,V_stack_l_all = np.linalg.svd(np.log10(bode_stacked_all),full_matrices=False)\n",
    "U_stack_l_12_7,S_stack_l_12_7,V_stack_l_12_7 = np.linalg.svd(np.log10(bode_stacked_12_7),full_matrices=False)\n",
    "U_stack_l_3_15,S_stack_l_3_15,V_stack_l_3_15 = np.linalg.svd(np.log10(bode_stacked_3_15),full_matrices=False)\n",
    "U_stack_l_3_15_7,S_stack_l_3_15_7,V_stack_l_3_15_7 = np.linalg.svd(np.log10(bode_stacked_3_15_7),full_matrices=False)\n",
    "U_stack_l_3_7,S_stack_l_3_7,V_stack_l_3_7 = np.linalg.svd(np.log10(bode_stacked_3_7),full_matrices=False)\n",
    "U_stack_l_3_6,S_stack_l_3_6,V_stack_l_3_6 = np.linalg.svd(np.log10(bode_stacked_3_6),full_matrices=False)\n",
    "U_stack_l_6_7,S_stack_l_6_7,V_stack_l_6_7 = np.linalg.svd(np.log10(bode_stacked_6_7),full_matrices=False)\n",
    "U_stack_l_3_12,S_stack_l_3_12,V_stack_l_3_12 = np.linalg.svd(np.log10(bode_stacked_3_12),full_matrices=False)\n",
    "U_stack_l_7_15,S_stack_l_7_15,V_stack_l_7_15 = np.linalg.svd(np.log10(bode_stacked_7_15),full_matrices=False)\n",
    "U_stack_l_12_15,S_stack_l_12_15,V_stack_l_12_15 = np.linalg.svd(np.log10(bode_stacked_12_15),full_matrices=False)\n",
    "\n",
    "U_stack_l_3,S_stack_l_3,V_stack_l_3 = np.linalg.svd(np.log10(c3),full_matrices=False)\n",
    "U_stack_l_12,S_stack_l_12,V_stack_l_12 = np.linalg.svd(np.log10(c12),full_matrices=False)\n",
    "U_stack_l_6,S_stack_l_6,V_stack_l_6 = np.linalg.svd(np.log10(c6),full_matrices=False)\n",
    "U_stack_l_c,S_stack_l_c,V_stack_l_c = np.linalg.svd(np.log10(bode_stacked_c),full_matrices=False)\n",
    "U_stack_l_7,S_stack_l_7,V_stack_l_7 = np.linalg.svd(np.log10(c7),full_matrices=False)\n",
    "U_stack_l_15,S_stack_l_15,V_stack_l_15 = np.linalg.svd(np.log10(c15),full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of labels\n",
    "snow_types0 = [0]*6*5 #Groomer\n",
    "snow_types1 = [1]*2*5 #Powder\n",
    "snow_types2 = [2]*1*5 #Icy\n",
    "snow_types3 = [3]*4*5 #Slushy\n",
    "snow_types = np.hstack((snow_types0,snow_types1,snow_types2,snow_types3))\n",
    "\n",
    "snow_types_stacked_all = np.tile(snow_types,(1,np.shape(bode_stacked_all)[1]//65))\n",
    "snow_types_stacked_all = pd.Series(snow_types_stacked_all[0,:])\n",
    "\n",
    "snow_types_stacked_12_7 = np.tile(snow_types,(1,np.shape(bode_stacked_12_7)[1]//65))\n",
    "snow_types_stacked_12_7 = pd.Series(snow_types_stacked_12_7[0,:])\n",
    "\n",
    "snow_types_stacked_3_15 = np.tile(snow_types,(1,np.shape(bode_stacked_3_15)[1]//65))\n",
    "snow_types_stacked_3_15 = pd.Series(snow_types_stacked_3_15[0,:])\n",
    "\n",
    "snow_types_stacked_3_7 = np.tile(snow_types,(1,np.shape(bode_stacked_3_7)[1]//65))\n",
    "snow_types_stacked_3_7 = pd.Series(snow_types_stacked_3_7[0,:])\n",
    "\n",
    "snow_types_stacked_3_15_7 = np.tile(snow_types,(1,np.shape(bode_stacked_3_15_7)[1]//65))\n",
    "snow_types_stacked_3_15_7 = pd.Series(snow_types_stacked_3_15_7[0,:])\n",
    "\n",
    "snow_types_stacked_3_12 = np.tile(snow_types,(1,np.shape(bode_stacked_3_12)[1]//65))\n",
    "snow_types_stacked_3_12 = pd.Series(snow_types_stacked_3_12[0,:])\n",
    "\n",
    "snow_types_stacked_7_15 = np.tile(snow_types,(1,np.shape(bode_stacked_7_15)[1]//65))\n",
    "snow_types_stacked_7_15 = pd.Series(snow_types_stacked_7_15[0,:])\n",
    "\n",
    "snow_types_stacked_c = np.tile(snow_types,(1,np.shape(bode_stacked_c)[1]//65))\n",
    "snow_types_stacked_c = pd.Series(snow_types_stacked_c[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cycle through random 70% training sets and calculate recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
      "<ipython-input-11-2a9a001058c0>:29: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n"
     ]
    }
   ],
   "source": [
    "# cycle through different class performances but only use 5 features.\n",
    "# Cycle through some other random 30% test data\n",
    "class_prec_df = pd.DataFrame(columns=['Groomed','Powder','Icy','Slushy']) # data frame for the class precision.\n",
    "class_recall_df = pd.DataFrame(columns=['Groomed','Powder','Icy','Slushy']) # data frame for the class recall.\n",
    "perf_array = []\n",
    "location_array = []\n",
    "labels_use = snow_types_stacked_c\n",
    "n_features = 5\n",
    "v_use = V_stack_l_15\n",
    "for i in range(1,500):\n",
    "    rs = i\n",
    "    labels = labels_use\n",
    "    performance = []\n",
    "    # Split our data\n",
    "    train, test, train_labels, test_labels = train_test_split(v_use[0:n_features,:].T,labels,test_size=0.3,random_state=rs)\n",
    "    # Initialize our classifier\n",
    "    gnb = GaussianNB()\n",
    "    # Train our classifier\n",
    "    model = gnb.fit(train, train_labels)\n",
    "    # Make predictions\n",
    "    preds = gnb.predict(test)\n",
    "    # Evaluate accuracy\n",
    "    p = accuracy_score(test_labels, preds)\n",
    "    # Append data\n",
    "    performance.append(p*100)\n",
    "    # Create confusion matrix\n",
    "    conf_matrix = metrics.confusion_matrix(test_labels, preds)\n",
    "    for j in range(0,len(conf_matrix)):\n",
    "        data = round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),3)\n",
    "        data1 = round(conf_matrix[j,j]/np.sum(conf_matrix[j,:]),3)\n",
    "        if np.isnan(data):\n",
    "            data = 0\n",
    "        elif np.isnan(data1):\n",
    "            data1 = 0\n",
    "        class_prec_df.loc[i,class_prec_df.columns[j]] = data*100\n",
    "        class_recall_df.loc[i,class_recall_df.columns[j]] = data1*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Plot the maximum performance after cycling through using multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a box plot for the class precision\n",
    "ylim = [40,105]\n",
    "xlim = [0,110]\n",
    "alpha = 0.6\n",
    "f_size = 8\n",
    "fig, ax = plt.subplots(1,1,figsize=[2,2], dpi=200)\n",
    "fig.patch.set_facecolor('white')\n",
    "color = ['royalblue','red','lightcoral','purple']\n",
    "sns.pointplot(data=class_recall_df, palette=color,ci=99,errwidth=1.25,scale=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "# ax.set_xlim(xlim)\n",
    "# ax.set_title('Class precision, using ' + str(n_features) + ' features', fontsize=f_size)\n",
    "# ax.set_ylabel('decimal Correct', fontsize=f_size)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.xaxis.set_ticklabels([])\n",
    "plt.yticks(fontsize=f_size)\n",
    "plt.xticks(fontsize=f_size)\n",
    "# plt.savefig(\"sg15_box_recall.svg\",bbox_inches='tight',dpi=fig.dpi)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
